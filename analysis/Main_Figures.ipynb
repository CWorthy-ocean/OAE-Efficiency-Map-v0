{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6996cb-5645-4315-a817-e49253051f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# pip install xarray\n",
    "# pip install cmocean\n",
    "# pip install cartopy\n",
    "# pip install pop-tools\n",
    "# pip install scipy\n",
    "# pip install gsw\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.image import imread\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cmocean\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "import pop_tools\n",
    "\n",
    "#import oae_smyle\n",
    "import util\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "import random\n",
    "import importlib\n",
    "import gsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29990b3b-4fcc-46f7-aa00-874c1dbad5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name = 'POP_gx1v7'\n",
    "grid = pop_tools.get_grid(grid_name)\n",
    "tlong = grid.TLONG.values\n",
    "tlat = grid.TLAT.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb25c15-8da9-4a00-bdaf-c2c15695b885",
   "metadata": {},
   "source": [
    "The following code demonstrates how to directly read a case of model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a9bf2-9d4a-4882-a130-b166ae3e439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case(region='Pacific', season='Jan', polygon=33):\n",
    "    \n",
    "    '''\n",
    "    To generate case name, given:\n",
    "        region: Pacific, Atlantic, South\n",
    "        season: Jan, Apr, Jul, Oct\n",
    "        polygon: polygon index, starting from 0\n",
    "    '''\n",
    "    \n",
    "    season_to_index = {'Jan': 0, 'Apr': 1, 'Jul': 2, 'Oct': 3}\n",
    "    region_to_alk_forcing_spec = {'Pacific': 'North_Pacific_basin', 'Atlantic': 'North_Atlantic_basin', 'South': 'South', 'Southern_Ocean': 'Southern_Ocean'}\n",
    "\n",
    "    # get foring index in this region\n",
    "    if season in season_to_index:\n",
    "        forcing_id = (polygon) * 4 + season_to_index[season]\n",
    "    else:\n",
    "        raise TypeError(\"Input season as: Jan, Apr, Jul, Oct\")\n",
    "\n",
    "    # all forcing files\n",
    "    alk_forcing_files = glob(f'/glade/scratch/mengyangz/oae-dor-global-efficiency/data/*{region}*')\n",
    "    alk_forcing_files.sort()\n",
    "    for i in range(len(alk_forcing_files)):\n",
    "        alk_forcing_files[i] = os.path.basename(alk_forcing_files[i][:-3])\n",
    "\n",
    "    if region in region_to_alk_forcing_spec:\n",
    "        alk_forcing_spec = region_to_alk_forcing_spec[region]\n",
    "    else:\n",
    "        raise TypeError(\"Input region as: Pacific, Atlantic, South\")\n",
    "    alk_forcing = alk_forcing_files[forcing_id]\n",
    "    case = f\"smyle-fosi.{alk_forcing_spec}.{alk_forcing}\"\n",
    "\n",
    "    return case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302b030-01ec-478e-834b-f66f668bc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oae_eff_curve(region='Pacific', season='Jan', polygon=9):\n",
    "    \n",
    "    '''\n",
    "    To generate case name, given:\n",
    "        region: Pacific, Atlantic, South\n",
    "        season: Jan, Apr, Jul, Oct\n",
    "        polygon: polygon index, starting from 0\n",
    "    '''\n",
    "    \n",
    "    oae_eff_curves_global = xr.open_dataset('./data/all_curves_global.nc')\n",
    "    # get foring index in this region\n",
    "    season_to_index = {'Jan': 0, 'Apr': 1, 'Jul': 2, 'Oct': 3}\n",
    "    \n",
    "    return oae_eff_curves_global.isel(season=season_to_index[season]).sel(region=region, polygon=polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba160899-4cda-46f3-86a8-9a72146c12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(region='Atlantic', season='Jan', polygon=1):\n",
    "\n",
    "    '''return dataset for a case'''\n",
    "    case = get_case(region=region, season=season, polygon=polygon)\n",
    "    print(case)\n",
    "\n",
    "    ds = oae_smyle.open_dataset(case, stream='pop.h')\n",
    "    ds = util.process(ds)\n",
    "\n",
    "    oae_result = get_oae_eff_curve(region=region, season=season, polygon=polygon)\n",
    "\n",
    "    # vertical distribution of excess ALK\n",
    "    alk_invertical = (ds.ALK_excess * ds.dz * ds.TAREA).sum(['nlat', 'nlon']) * 1e-9  # 1e3 meq, should be 1e-6 for meq\n",
    "    alk_invertical = alk_invertical.compute()\n",
    "    alk_total = alk_invertical.sum(['z_t'])  # meq\n",
    "    frac_alk_invertical = alk_invertical / alk_total\n",
    "    frac_alk_invertical = frac_alk_invertical.compute()\n",
    "    \n",
    "    return ds, oae_result, alk_invertical, frac_alk_invertical, alk_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb0113-2061-4951-9c83-2f61adec2f3b",
   "metadata": {},
   "source": [
    "Read a case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfee03-a1fb-4730-b5e4-344e37d0362b",
   "metadata": {},
   "source": [
    "High latitude North Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98dd6a-42f3-4d01-a6e2-91e61409bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds, oae_result, alk_invertical, frac_alk_invertical, alk_total = get_ds(region='Atlantic', season='Jan', polygon=0)\n",
    "ds_apr, oae_result_apr, alk_invertical_apr, frac_alk_invertical_apr, alk_total_apr = get_ds(region='Atlantic', season='Apr', polygon=0)\n",
    "ds_jul, oae_result_jul, alk_invertical_jul, frac_alk_invertical_jul, alk_total_jul = get_ds(region='Atlantic', season='Jul', polygon=0)\n",
    "ds_oct, oae_result_oct, alk_invertical_oct, frac_alk_invertical_oct, alk_total_oct = get_ds(region='Atlantic', season='Oct', polygon=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100c539-1c9f-47d2-ac9c-cf33a090c595",
   "metadata": {},
   "source": [
    "Gulf of St. Lawrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d89d0-199b-4676-a046-59188821ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_63, oae_result_63, alk_invertical_63, frac_alk_invertical_63, alk_total_63 = get_ds(region='Atlantic', season='Jan', polygon=63)\n",
    "ds_apr_63, oae_result_apr_63, alk_invertical_apr_63, frac_alk_invertical_apr_63, alk_total_apr_63 = get_ds(region='Atlantic', season='Apr', polygon=63)\n",
    "ds_jul_63, oae_result_jul_63, alk_invertical_jul_63, frac_alk_invertical_jul_63, alk_total_jul_63 = get_ds(region='Atlantic', season='Jul', polygon=63)\n",
    "ds_oct_63, oae_result_oct_63, alk_invertical_oct_63, frac_alk_invertical_oct_63, alk_total_oct_63 = get_ds(region='Atlantic', season='Oct', polygon=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774e150-0731-4cac-a968-d2ba50ca1e3b",
   "metadata": {},
   "source": [
    "Subtropical North Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b866a-af6e-4a54-9084-9751415a2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_142, oae_result_142, alk_invertical_142, frac_alk_invertical_142, alk_total_142 = get_ds(region='Atlantic', season='Jan', polygon=142)\n",
    "ds_apr_142, oae_result_apr_142, alk_invertical_apr_142, frac_alk_invertical_apr_142, alk_total_apr_142 = get_ds(region='Atlantic', season='Apr', polygon=142)\n",
    "ds_jul_142, oae_result_jul_142, alk_invertical_jul_142, frac_alk_invertical_jul_142, alk_total_jul_142 = get_ds(region='Atlantic', season='Jul', polygon=142)\n",
    "ds_oct_142, oae_result_oct_142, alk_invertical_oct_142, frac_alk_invertical_oct_142, alk_total_oct_142 = get_ds(region='Atlantic', season='Oct', polygon=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcc127-1cae-414f-9ed0-48d3a8ba417e",
   "metadata": {},
   "source": [
    "Equatorial Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e78c1-caea-4ce9-8bad-45c7c3338ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_129, oae_result_129, alk_invertical_129, frac_alk_invertical_129, alk_total_129 = get_ds(region='Atlantic', season='Jan', polygon=129)\n",
    "ds_apr_129, oae_result_apr_129, alk_invertical_apr_129, frac_alk_invertical_apr_129, alk_total_apr_129 = get_ds(region='Atlantic', season='Apr', polygon=129)\n",
    "ds_jul_129, oae_result_jul_129, alk_invertical_jul_129, frac_alk_invertical_jul_129, alk_total_jul_129 = get_ds(region='Atlantic', season='Jul', polygon=129)\n",
    "ds_oct_129, oae_result_oct_129, alk_invertical_oct_129, frac_alk_invertical_oct_129, alk_total_oct_129 = get_ds(region='Atlantic', season='Oct', polygon=129)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8114f-9aa1-4dbc-aa36-2bccec963f60",
   "metadata": {},
   "source": [
    "# Read curves directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486b117-c36d-4d85-871e-175d663030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('./data/Plumes_data/pCO2_SURF_excess_0_Atlantic.nc')\n",
    "ds_63 = xr.open_dataset('./data/Plumes_data/pCO2_SURF_excess_63_Atlantic.nc')\n",
    "ds_142 = xr.open_dataset('./data/Plumes_data/pCO2_SURF_excess_142_Atlantic.nc')\n",
    "d_129 = xr.open_dataset('./data/Plumes_data/pCO2_SURF_excess_129_Atlantic.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c119d07-788c-4da6-a4b3-1bdfa15db98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_alk_invertical = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_0_Atlantic.nc')\n",
    "frac_alk_invertical_apr = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_apr_0_Atlantic.nc')\n",
    "frac_alk_invertical_jul = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_jul_0_Atlantic.nc')\n",
    "frac_alk_invertical_oct = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_oct_0_Atlantic.nc')\n",
    "\n",
    "frac_alk_invertical_63 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_63_Atlantic.nc')\n",
    "frac_alk_invertical_apr_63 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_apr_63_Atlantic.nc')\n",
    "frac_alk_invertical_jul_63 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_jul_63_Atlantic.nc')\n",
    "frac_alk_invertical_oct_63 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_oct_63_Atlantic.nc')\n",
    "\n",
    "frac_alk_invertical_142 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_142_Atlantic.nc')\n",
    "frac_alk_invertical_apr_142 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_apr_142_Atlantic.nc')\n",
    "frac_alk_invertical_jul_142 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_jul_142_Atlantic.nc')\n",
    "frac_alk_invertical_oct_142 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_oct_142_Atlantic.nc')\n",
    "\n",
    "frac_alk_invertical_129 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_129_Atlantic.nc')\n",
    "frac_alk_invertical_apr_129 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_apr_129_Atlantic.nc')\n",
    "frac_alk_invertical_jul_129 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_jul_129_Atlantic.nc')\n",
    "frac_alk_invertical_oct_129 = xr.open_dataset('./data/Plumes_data/frac_alk_invertical_oct_129_Atlantic.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2379b7-ef4e-463b-a0a4-52787e024bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "oae_result = get_oae_eff_curve(region='Atlantic', season='Jan', polygon=0)\n",
    "oae_result_apr = get_oae_eff_curve(region='Atlantic', season='Apr', polygon=0)\n",
    "oae_result_jul = get_oae_eff_curve(region='Atlantic', season='Jul', polygon=0)\n",
    "oae_result_oct = get_oae_eff_curve(region='Atlantic', season='Oct', polygon=0)\n",
    "\n",
    "oae_result_63 = get_oae_eff_curve(region='Atlantic', season='Jan', polygon=63)\n",
    "oae_result_apr_63 = get_oae_eff_curve(region='Atlantic', season='Apr', polygon=63)\n",
    "oae_result_jul_63 = get_oae_eff_curve(region='Atlantic', season='Jul', polygon=63)\n",
    "oae_result_oct_63 = get_oae_eff_curve(region='Atlantic', season='Oct', polygon=63)\n",
    "\n",
    "oae_result_142 = get_oae_eff_curve(region='Atlantic', season='Jan', polygon=142)\n",
    "oae_result_apr_142 = get_oae_eff_curve(region='Atlantic', season='Apr', polygon=142)\n",
    "oae_result_jul_142 = get_oae_eff_curve(region='Atlantic', season='Jul', polygon=142)\n",
    "oae_result_oct_142 = get_oae_eff_curve(region='Atlantic', season='Oct', polygon=142)\n",
    "\n",
    "oae_result_129 = get_oae_eff_curve(region='Atlantic', season='Jan', polygon=129)\n",
    "oae_result_apr_129 = get_oae_eff_curve(region='Atlantic', season='Apr', polygon=129)\n",
    "oae_result_jul_129 = get_oae_eff_curve(region='Atlantic', season='Jul', polygon=129)\n",
    "oae_result_oct_129 = get_oae_eff_curve(region='Atlantic', season='Oct', polygon=129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf73e6f-38fc-4b40-acbe-d5e09255b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = [ds, ds_63, ds_142, ds_129]\n",
    "\n",
    "oae_effs = [ [oae_result, oae_result_apr, oae_result_jul, oae_result_oct],\\\n",
    "[oae_result_63, oae_result_apr_63, oae_result_jul_63, oae_result_oct_63],\\\n",
    "[oae_result_142, oae_result_apr_142, oae_result_jul_142, oae_result_oct_142],\\\n",
    "[oae_result_129, oae_result_apr_129, oae_result_jul_129, oae_result_oct_129] ]\n",
    "\n",
    "fra_alks = [ [frac_alk_invertical, frac_alk_invertical_apr, frac_alk_invertical_jul, frac_alk_invertical_oct],\\\n",
    "[frac_alk_invertical_63, frac_alk_invertical_apr_63, frac_alk_invertical_jul_63, frac_alk_invertical_oct_63],\\\n",
    "[frac_alk_invertical_142, frac_alk_invertical_apr_142, frac_alk_invertical_jul_142, frac_alk_invertical_oct_142],\\\n",
    "[frac_alk_invertical_129, frac_alk_invertical_apr_129, frac_alk_invertical_jul_129, frac_alk_invertical_oct_129] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae884f25-5823-451d-86f1-ed5bb241b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlantic polygon masks\n",
    "final_polygon_mask_atlantic = np.load('./data/polygon_data/Atlantic_final_polygon_mask.npy')\n",
    "final_polygon_vertices_atlantic = np.load('./data/polygon_data/Atlantic_final_polygon_vertices.npy', allow_pickle=True)\n",
    "cluster_centers_atlantic = np.load('./data/polygon_data/Atlantic_final_cluster_centers.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745541f8-4fec-42a0-8904-d8ff01962cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, ii):\n",
    "    # a list of colors\n",
    "    # colorsss = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # ind_color = np.arange(len(colorsss)) # 0- 9\n",
    "\n",
    "    vertices = np.array(final_polygon_vertices_atlantic[ii])\n",
    "    # plot convex hull\n",
    "    if len(vertices) >= 3:\n",
    "        hull = ConvexHull(vertices) # get the indices of the periphery of a patch, for plotting purpose\n",
    "        polygon = list(vertices[iii] for iii in hull.vertices.tolist() )\n",
    "        polygon = np.array(polygon)\n",
    "\n",
    "        ax.plot(np.append(polygon[:,0], polygon[0,0]), np.append(polygon[:,1], polygon[0,1]), 'k-', linewidth=0.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # plot polygon masks\n",
    "    index = np.where(final_polygon_mask_atlantic[ii] == 1)\n",
    "    ax.scatter(tlong[index], tlat[index], c='k', s=1, alpha=0.6, transform=ccrs.PlateCarree())\n",
    "    #ax.text(cluster_centers_atlantic[ii, 0]-2, cluster_centers_atlantic[ii, 1]-1,str(ii), fontsize=9, color='k', transform=ccrs.PlateCarree())\n",
    "\n",
    "def no_nans(array):\n",
    "    return array[~np.isnan(array)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6248d-2b4d-4d92-bf8d-768dc025cc68",
   "metadata": {},
   "source": [
    "### Adding weighted mixed layer depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f743272-c840-40cd-ab16-102791cac43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_ind = [0, 63, 142, 129]\n",
    "season = [0,1,2,3]\n",
    "forcing_ind = np.zeros((4, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        forcing_ind[i,j] = poly_ind[i]*4 + season[j]\n",
    "\n",
    "eff_mld = np.zeros((4, 4, 3))  # number of polygons, number of season, number of years\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        res_file = xr.open_dataset(f'./data/Atlantic_weighted_FG_CO2_abs/Atlantic{int(forcing_ind[i,j]):04d}.nc')\n",
    "        eff_mld[i,j,0] = res_file.mld_weight_1.values[12*1]/100 # year 1, no need for offset by season, because it was all 180 mths\n",
    "        eff_mld[i,j,1] = res_file.mld_weight_1.values[12*2]/100\n",
    "        eff_mld[i,j,2] = res_file.mld_weight_1.values[12*8]/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945eb439-a932-4236-ada9-776e15c4c4da",
   "metadata": {},
   "source": [
    "# Fig. 5\n",
    "Decomposition of gas exchange timescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17017d5d-e200-4ced-b98a-e50444e98c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_curves_global = xr.open_dataset('./data/all_curves_global.nc', decode_times=False)\n",
    "all_curves_global_tau_FG_CO2 = xr.open_dataset('./data/all_curves_global_tau_FG_CO2.nc', decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1ae76-ad18-4472-acfc-a4364c485545",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "nrow = 4\n",
    "ncol = 4\n",
    "\n",
    "fig = plt.figure(figsize=(16,14))\n",
    "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[1, 1, 1, 1])\n",
    "\n",
    "colors = ['blue', 'cyan', 'red', 'orange']\n",
    "labels = ['Jan', 'Apr', 'Jul', 'Oct']\n",
    "lon_mins = [-80, -80, -80, -80]\n",
    "lon_maxs = [25, 25, 30, 30]\n",
    "lat_mins = [0, 0, 0, -40]\n",
    "lat_maxs = [80, 80, 80, 40]\n",
    "central_longitudes = [0, 0, 0, 0]\n",
    "\n",
    "reg = 'Atlantic'\n",
    "polys = [0, 63, 142, 129]\n",
    "\n",
    "\n",
    "#text_position = [750,95,125,39.5]\n",
    "text_position = [870,110,140,43]\n",
    "text_position_1 = [870,110,140,43]\n",
    "ll = ['a', 'b', 'c', 'd']\n",
    "region_names = ['Labrador Sea', 'St. Lawrence Gulf', 'Subtrop. N. Atlantic', 'Equat. Atlantic']\n",
    "\n",
    "for i in range(nrow):\n",
    "    \n",
    "    surf_dil = all_curves_global.sel(region=reg, polygon=polys[i])  # surface dilution curves for a polygon, with 4 seasons\n",
    "    tau_comp = all_curves_global_tau_FG_CO2.sel(region=reg, polygon=polys[i]) # tau gax componets for a region\n",
    "    \n",
    "    for j in range(ncol): \n",
    "\n",
    "        ################ tau_gas_effective_MLD\n",
    "        if j == 0:\n",
    "            ax = fig.add_subplot(nrow, ncol, i*ncol+j+1)\n",
    "            ax.text(-5, text_position[i], f' {ll[i]}, i', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            tarray = np.arange(0, 180, 1)/12\n",
    "            \n",
    "            for k in range(4):\n",
    "                ax.plot(tarray, no_nans(tau_comp.isel(season=k).tau_gas_approxi_weight_1_effectiveMLD.values/30), linewidth=1.5, color=colors[k], alpha=0.1)\n",
    "                ind_not_nan = ~np.isnan(tau_comp.isel(season=k).tau_gas_approxi_weight_1_effectiveMLD.values)\n",
    "                ax.plot(tarray, tau_comp.isel(season=k).tau_gas_approxi_weight_1_effectiveMLD.rolling(time=12, min_periods=12, center=True).mean().values[ind_not_nan]/30, linewidth=2.5, color=colors[k], label=labels[k], alpha=0.6)\n",
    "\n",
    "                #ax.set_ylim(7, 26)\n",
    "                ax.set_ylabel('τ (months)')\n",
    "                ax.text(-0.5, 0.5, f'{region_names[i]}', ha='center', va='center', rotation=90, transform=ax.transAxes, fontsize=17)  \n",
    "                #ax.text(-8, text_position[i]/3, f'{region_names[i]}', rotation=90)\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Time since release (years)')\n",
    "                custom_x_ticks = [0, 1, 2, 4, 8, 15]\n",
    "                custom_x_labels  = [str(num) for num in custom_x_ticks]\n",
    "                ax.set_xticks(custom_x_ticks)\n",
    "                ax.set_xticklabels(custom_x_labels);\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "\n",
    "            # add vertical lines\n",
    "            years_highlight = [1, 2, 8]\n",
    "            for p in range(len(years_highlight)):\n",
    "                ax.axvline(years_highlight[p], linestyle='--', color='k',linewidth=1, )\n",
    "        \n",
    "        ################ surf_dil\n",
    "        elif j == 1:\n",
    "            ax = fig.add_subplot(nrow, ncol, i*ncol+j+1)\n",
    "            ax.text(-4, 0.65, 'ii', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            tarray = np.arange(0, 180, 1)/12\n",
    "            \n",
    "            for k in range(4):\n",
    "                ax.plot(tarray, no_nans(surf_dil.isel(season=k).frac_ALK_excess_surf.values), linewidth=2.5, color=colors[k], label=labels[k], alpha=0.6)\n",
    "                ax.set_ylim(0, 0.6)\n",
    "                ax.set_ylabel('Frac. of added Alk in surf.')\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Time since release (years)')\n",
    "                custom_x_ticks = [0, 1, 2, 4, 8, 15]\n",
    "                custom_x_labels  = [str(num) for num in custom_x_ticks]\n",
    "                ax.set_xticks(custom_x_ticks)\n",
    "                ax.set_xticklabels(custom_x_labels);\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "            if i == 0:\n",
    "                ax.legend(loc='upper right')\n",
    "\n",
    "            # add vertical lines\n",
    "            years_highlight = [1, 2, 8]\n",
    "            for p in range(len(years_highlight)):\n",
    "                ax.axvline(years_highlight[p], linestyle='--', color='k',linewidth=1, )\n",
    "\n",
    "\n",
    "        ################ piston velocity\n",
    "        elif j == 2:\n",
    "            ax = fig.add_subplot(nrow, ncol, i*ncol+j+1)\n",
    "            ax.text(-4, 13, 'iii', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            tarray = np.arange(0, 180, 1)/12\n",
    "            \n",
    "            for k in range(4):\n",
    "                ax.plot(tarray, no_nans(tau_comp.isel(season=k).piston_weight_1.values/100*86400), linewidth=1.5, color=colors[k], label=labels[k], alpha=0.1)\n",
    "                ind_not_nan = ~np.isnan(tau_comp.isel(season=k).piston_weight_1.values)\n",
    "                ax.plot(tarray, tau_comp.isel(season=k).piston_weight_1.rolling(time=12, min_periods=12, center=True).mean().values[ind_not_nan]/100*86400, linewidth=2.5, color=colors[k], label=labels[k], alpha=0.6)\n",
    "\n",
    "                ax.set_ylim(0, 12)\n",
    "                ax.set_ylabel('$k$ (m/day)')\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Time since release (years)')\n",
    "                custom_x_ticks = [0, 1, 2, 4, 8, 15]\n",
    "                custom_x_labels  = [str(num) for num in custom_x_ticks]\n",
    "                ax.set_xticks(custom_x_ticks)\n",
    "                ax.set_xticklabels(custom_x_labels);\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "\n",
    "            # add vertical lines\n",
    "            years_highlight = [1, 2, 8]\n",
    "            for p in range(len(years_highlight)):\n",
    "                ax.axvline(years_highlight[p], linestyle='--', color='k',linewidth=1, )\n",
    "                \n",
    "        ################ dDIC_dCO2\n",
    "        elif j == 3:\n",
    "            ax = fig.add_subplot(nrow, ncol, i*ncol+j+1)\n",
    "            ax.text(-4, 29, 'iv', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            tarray = np.arange(0, 180, 1)/12\n",
    "            \n",
    "            for k in range(4):\n",
    "                ax.plot(tarray, no_nans(tau_comp.isel(season=k).dDICdCO2_approxi_weight_1.values), linewidth=1.5, color=colors[k], label=labels[k], alpha=0.1)\n",
    "                ind_not_nan = ~np.isnan(tau_comp.isel(season=k).dDICdCO2_approxi_weight_1.values)\n",
    "                ax.plot(tarray, tau_comp.isel(season=k).dDICdCO2_approxi_weight_1.rolling(time=12, min_periods=12, center=True).mean().values[ind_not_nan], linewidth=2.5, color=colors[k], label=labels[k], alpha=0.6)\n",
    "\n",
    "                ax.set_ylim(7, 28)\n",
    "                ax.set_ylabel('β')\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Time since release (years)')\n",
    "                custom_x_ticks = [0, 1, 2, 4, 8, 15]\n",
    "                custom_x_labels  = [str(num) for num in custom_x_ticks]\n",
    "                ax.set_xticks(custom_x_ticks)\n",
    "                ax.set_xticklabels(custom_x_labels);\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "\n",
    "            # add vertical lines\n",
    "            years_highlight = [1, 2, 8]\n",
    "            for p in range(len(years_highlight)):\n",
    "                ax.axvline(years_highlight[p], linestyle='--', color='k',linewidth=1, )\n",
    "                \n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.2)\n",
    "\n",
    "#plt.savefig('./Figurex_tau_gas_components_4x4.png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c2370-17ef-4208-b66d-64a1dce22aa7",
   "metadata": {},
   "source": [
    "# Adding histograms of CO2 uptake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453bdb6-81ac-4c50-abc4-f818c098acff",
   "metadata": {},
   "source": [
    "### Distances of each grid point to a polygon center, assign each grid point to a ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a938cf-2cbd-4330-8220-7f829883c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name = 'POP_gx1v7'\n",
    "grid = pop_tools.get_grid(grid_name)\n",
    "tlong = grid.TLONG.values\n",
    "tlat = grid.TLAT.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8d4f6-6135-481a-87a6-b29adecaa227",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlong_conv = np.where(tlong > 180, tlong - 360, tlong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a1155-f97c-4b96-839c-c126805995a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlantic\n",
    "final_polygon_mask_atlantic = np.load('./data/polygon_data/Atlantic_final_polygon_mask.npy')\n",
    "final_polygon_vertices_atlantic = np.load('./data/polygon_data/Atlantic_final_polygon_vertices.npy', allow_pickle=True)\n",
    "cluster_centers_atlantic = np.load('./data/polygon_data/Atlantic_final_cluster_centers.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b940cd-474a-43d8-9ef7-b773d2fcf09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dist(target):    \n",
    "    '''\n",
    "    target: [lon, lat]\n",
    "    '''\n",
    "\n",
    "    n, m = tlong_conv.shape\n",
    "\n",
    "    dist = np.zeros_like(tlong_conv)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            dist[i,j] = gsw.distance([tlong_conv[i,j], target[0]], [tlat[i,j], target[1]])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48be97-9b27-4437-9526-a5aa3cce928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dist_0 = cal_dist(cluster_centers_atlantic[0])\n",
    "dist_63 = cal_dist(cluster_centers_atlantic[63])\n",
    "dist_142 = cal_dist(cluster_centers_atlantic[142])\n",
    "dist_129 = cal_dist(cluster_centers_atlantic[129])\n",
    "\n",
    "dist_5 = cal_dist(cluster_centers_atlantic[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfbb74-3e75-4dcb-b97a-ef73480b974b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_rings(dist_0, num_rings=100):\n",
    "\n",
    "    '''\n",
    "    Return ring matrix, assign each grid point to a certain ring.\n",
    "    bin_edges: distances\n",
    "    '''\n",
    "    \n",
    "    # Determine the range of distances\n",
    "    min_distance = np.min(dist_0)\n",
    "    max_distance = np.max(dist_0)\n",
    "\n",
    "    # Define the bin edges to create rings\n",
    "    #bin_edges = np.concatenate((np.arange(0, 2000*1e3, 50*1e3), np.arange(2000*1e3, max_distance, 100*1e3)))\n",
    "    bin_edges = np.arange(0, 4100*1e3, 50*1e3)\n",
    "\n",
    "    # Use digitize to assign each point to a ring\n",
    "    rings = np.digitize(dist_0, bin_edges, right=True)\n",
    "    \n",
    "    return bin_edges, rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ddc39-03b8-4431-bc6d-d179f7417064",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_0, rings_0 = make_rings(dist_0)\n",
    "bin_edges_63, rings_63 = make_rings(dist_63)\n",
    "bin_edges_142, rings_142 = make_rings(dist_142)\n",
    "bin_edges_129, rings_129 = make_rings(dist_129)\n",
    "\n",
    "bin_edges_5, rings_5 = make_rings(dist_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074ee2c-e268-4492-9ba8-a51b6c649f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tlong, tlat, c=rings_5, cmap='rainbow', vmin=0, vmax=10)\n",
    "plt.colorbar()\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(-25,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789ccd3-aa72-4b63-9bcd-3fa553b65425",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rings_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf33c7-847d-45df-9633-51f90dc22698",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = xr.open_dataset('./data/Plumes_FG_CO2_histograms/Southern_Ocean-0032.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e5bd39-605e-47fa-95b7-a0db2e7faa08",
   "metadata": {},
   "source": [
    "FG_CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952613d-68b4-4a96-9e77-8a985c64c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = './data/FG_CO2_excess/'\n",
    "FG_CO2_excess = xr.open_dataset(fpath + 'North_Atlantic_basin-0003.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023759fb-deaf-4748-a21b-2154314f64c7",
   "metadata": {},
   "source": [
    "FG_CO2: mmol/m3 cm/s\n",
    "\n",
    "TAREA: cm2\n",
    "\n",
    "time_delta: days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dd550-0987-4218-94c8-a94b9f112399",
   "metadata": {},
   "outputs": [],
   "source": [
    "FG_CO2_excess_area_time = (FG_CO2_excess.FG_CO2_excess * FG_CO2_excess.TAREA * FG_CO2_excess.time_delta) / 1e6 * 86400 # mmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66b299-4b88-481e-ae6d-4e21f69945ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total CO2 uptake in each ring, and each time\n",
    "\n",
    "num_time = len(FG_CO2_excess_area_time.time) # 180 time steps\n",
    "num_rings = len(bin_edges_0)\n",
    "\n",
    "FG_CO2_rings = np.zeros((num_time, num_rings))\n",
    "rings_ind = np.unique(rings_0)  # unique ring indices\n",
    "\n",
    "for i in range(num_rings):\n",
    "    ring_mask = np.where(rings_0 == rings_ind[i], 1, 0)  # 1 if in this ring\n",
    "    FG_CO2_rings[:, i] = (FG_CO2_excess_area_time * ring_mask).sum(dim=['nlat', 'nlon']) # mmol\n",
    "     \n",
    "total_FG_CO2 = FG_CO2_excess_area_time.sum(dim=['time', 'nlat', 'nlon']).values # total CO2 uptake in 15 years, mmol\n",
    "FG_CO2_rings_per = FG_CO2_rings / total_FG_CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2130cd5-df5f-4b01-a0c4-911567c72f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rs = [3,6,12,24,72,180]\n",
    "r=0\n",
    "stacks = []\n",
    "labels = []\n",
    "\n",
    "for rs in all_rs:\n",
    "    # sum individual histograms across the time axis into groups.\n",
    "    stacks.append(np.sum(FG_CO2_rings_per[r:rs],axis=0))\n",
    "    percentage = np.sum(FG_CO2_rings_per[r:rs])\n",
    "    labels.append(\"%.1f %% in months %d-%d\"%(percentage*100,r,rs))\n",
    "    # move to the next block\n",
    "    r=rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c328b06-4ca7-437a-bfa5-213055847a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a04bd2-0c49-490f-9c87-9d262fad5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stackplot(bin_edges_0/1000, stacks);\n",
    "plt.xlim(0, 4000)\n",
    "\n",
    "#plt.plot(bin_edges_0/1000, np.sum(FG_CO2_rings_per, axis=0), color='k', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883c0e7-b923-453c-970d-83b44d2918f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0e49c-7567-4da7-8125-40baca0fddd8",
   "metadata": {},
   "source": [
    "Put together in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8de4c1-e4dc-4726-9618-205da8b79d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cum_FG_CO2_ring(FG_CO2_excess, bin_edges_0, rings_0):\n",
    "    \n",
    "    '''\n",
    "    Return array of cummulative percentage CO2 uptake in each rings\n",
    "    '''\n",
    "    \n",
    "    # total CO2 uptake in each ring, and each time\n",
    "    \n",
    "    FG_CO2_excess_area_time = (FG_CO2_excess.FG_CO2_excess * FG_CO2_excess.TAREA * FG_CO2_excess.time_delta) / 1e6 * 86400 # mmol\n",
    "    total_FG_CO2 = FG_CO2_excess_area_time.sum(dim=['time', 'nlat', 'nlon']).values\n",
    "\n",
    "    num_time = len(FG_CO2_excess_area_time.time) # 180 time steps\n",
    "    num_rings = len(bin_edges_0)\n",
    "\n",
    "    FG_CO2_rings = np.zeros((num_time, num_rings))\n",
    "    rings_ind = np.unique(rings_0)  # unique ring indices\n",
    "\n",
    "    for i in range(num_rings):\n",
    "        ring_mask = np.where(rings_0 == rings_ind[i], 1, 0)  # 1 if in this ring\n",
    "        FG_CO2_rings[:, i] = (FG_CO2_excess_area_time * ring_mask).sum(dim=['nlat', 'nlon']) # mmol\n",
    "\n",
    "    #total_FG_CO2 = np.sum(FG_CO2_rings) # total CO2 uptake in 15 years, mmol\n",
    "    FG_CO2_rings_per = FG_CO2_rings / total_FG_CO2\n",
    "    \n",
    "    return FG_CO2_rings_per\n",
    "\n",
    "def get_cum_FG_CO2_ring_4seasons(polygons, bin_edges_0, rings_0):\n",
    "    \n",
    "    '''\n",
    "    Return array of cummulative percentage CO2 uptake in each rings, 4 seasons\n",
    "    '''\n",
    "    \n",
    "    FG_CO2_rings_per_4seasons = np.zeros((4, 180, len(bin_edges_0)))\n",
    "    \n",
    "    fpath = './data/FG_CO2_excess/'\n",
    "\n",
    "    ind = np.arange(polygons*4, polygons*4 + 4, 1)\n",
    "    for i in range(4):\n",
    "        FG_CO2_excess = xr.open_dataset(fpath + f'North_Atlantic_basin-{ind[i]:04d}.nc')\n",
    "        FG_CO2_rings_per_4seasons[i, :, :] = get_cum_FG_CO2_ring(FG_CO2_excess, bin_edges_0, rings_0)\n",
    "    \n",
    "    return FG_CO2_rings_per_4seasons\n",
    "\n",
    "def get_stack(FG_CO2_rings_per, long=False):\n",
    "\n",
    "    all_rs = [3,6,12,24,72,180]\n",
    "    r=0\n",
    "    stacks = []\n",
    "    labels = []\n",
    "\n",
    "    for rs in all_rs:\n",
    "        # sum individual histograms across the time axis into groups.\n",
    "        stacks.append(np.sum(FG_CO2_rings_per[r:rs],axis=0)*100)\n",
    "        percentage = np.sum(FG_CO2_rings_per[r:rs])\n",
    "        \n",
    "        if long==True:\n",
    "            labels.append(\"%.1f %% in months %d-%d\"%(percentage*100,r,rs))\n",
    "        else:\n",
    "            labels.append(\"%.1f %%\"%(percentage*100))\n",
    "        # move to the next block\n",
    "        r=rs\n",
    "        \n",
    "    return stacks, labels\n",
    "\n",
    "def find_element_index(arr, element_to_find):\n",
    "    \"\"\"\n",
    "    Find the index (or indices) of a specified element in a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "    - If the element is found:\n",
    "      - If the element appears multiple times, returns a NumPy array of indices.\n",
    "      - If the element appears only once, returns a single index.\n",
    "    - If the element is not found, returns None.\n",
    "    \"\"\"\n",
    "    indices = np.where(arr == element_to_find)[0]\n",
    "\n",
    "    if len(indices) == 0:\n",
    "        return None\n",
    "    elif len(indices) == 1:\n",
    "        return indices[0]\n",
    "    else:\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab76567-5daf-402f-8185-9a943ddc9cff",
   "metadata": {},
   "source": [
    "### Get all 4 seasons for all 4 polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3b980-6771-47c5-9b55-ca980645977d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "FG_CO2_rings_0 = get_cum_FG_CO2_ring_4seasons(0, bin_edges_0, rings_0)\n",
    "FG_CO2_rings_63 = get_cum_FG_CO2_ring_4seasons(63, bin_edges_63, rings_63)\n",
    "FG_CO2_rings_142 = get_cum_FG_CO2_ring_4seasons(142, bin_edges_142, rings_142)\n",
    "FG_CO2_rings_129 = get_cum_FG_CO2_ring_4seasons(129, bin_edges_129, rings_129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b13e6-f9f0-4637-951e-6dad0bba7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(FG_CO2_rings_0, p):\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            'FG_CO2_percent': (['season', 'time', 'dist2center'], FG_CO2_rings_0),\n",
    "        },\n",
    "        coords={'season': ['Januaray', 'April', 'July', 'October'],\n",
    "                'time': FG_CO2_excess.time.values,\n",
    "                'dist2center': bin_edges_0\n",
    "               },\n",
    "    )\n",
    "\n",
    "    ds = ds.expand_dims(polygon=[p])\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cbac5-23fa-44f8-8dc2-774edee432d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrs = [FG_CO2_rings_0, FG_CO2_rings_63, FG_CO2_rings_142, FG_CO2_rings_129]\n",
    "# pols = [0, 63, 142, 129]\n",
    "\n",
    "ds_0_ = get_ds(FG_CO2_rings_0, 0)\n",
    "ds_63_ = get_ds(FG_CO2_rings_63, 63)\n",
    "ds_142_ = get_ds(FG_CO2_rings_142, 142)\n",
    "ds_129_ = get_ds(FG_CO2_rings_129, 129)\n",
    "\n",
    "ds_4poly = xr.concat([ds_0_, ds_63_, ds_142_, ds_129_], dim='polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bdc9b-146e-4550-9aeb-d6125e6d8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4poly.to_netcdf('./data/Plumes_FG_CO2_percent.nc', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3657ce-41e8-4a66-a9d0-f4506c9270ad",
   "metadata": {},
   "source": [
    "# Read from here for any histogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8adac-d3c4-40b7-83dd-54045069d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4poly = xr.open_dataset('./data/Plumes_FG_CO2_percent.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8db1fe-a29d-4a95-9fcb-591631bba867",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29a10f-e406-40f0-ac10-672186aa30e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Map view of cumulative CO2 uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9953a8-41b2-45de-b1d4-f4918a617254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumul_CO2uptake_map(polygon):\n",
    "    \n",
    "    fpath = './data/FG_CO2_excess/'\n",
    "\n",
    "    ind = polygon*4\n",
    "    FG_CO2_excess = xr.open_dataset(fpath + f'North_Atlantic_basin-{ind:04d}.nc')\n",
    "    \n",
    "    FG_CO2_excess_area_time = (FG_CO2_excess.FG_CO2_excess * FG_CO2_excess.TAREA * FG_CO2_excess.time_delta) / 1e6 * 86400 # mmol\n",
    "    total_FG_CO2 = FG_CO2_excess_area_time.sum(dim=['time', 'nlat', 'nlon']).values # total CO2 uptake in 15 years, mmol\n",
    "\n",
    "    frac_FG_CO2 = FG_CO2_excess_area_time / total_FG_CO2*100\n",
    "    frac_FG_CO2_cumul = frac_FG_CO2.sum(['time'])\n",
    "    \n",
    "    frac_FG_CO2_cumul = frac_FG_CO2_cumul.to_dataset(name='frac_FG_CO2_cumul')\n",
    "    \n",
    "    return frac_FG_CO2_cumul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9f1a8-b7f4-445f-bfc6-5ca5e57f15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frac_FG_CO2_map_0 = get_cumul_CO2uptake_map(0)\n",
    "frac_FG_CO2_map_63 = get_cumul_CO2uptake_map(63)\n",
    "frac_FG_CO2_map_142 = get_cumul_CO2uptake_map(142)\n",
    "frac_FG_CO2_map_129 = get_cumul_CO2uptake_map(129)\n",
    "\n",
    "ds_frac_FG_CO2_map = [frac_FG_CO2_map_0, frac_FG_CO2_map_63, frac_FG_CO2_map_142, frac_FG_CO2_map_129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42289a-df84-4653-bbe3-1f8721fecb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_FG_CO2_map_0.frac_FG_CO2_cumul.max(), frac_FG_CO2_map_0.frac_FG_CO2_cumul.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35a55a-2ee0-482f-bffa-6500db8ee7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_FG_CO2_map_129.frac_FG_CO2_cumul.max().values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607803a-4e56-4c69-a2ee-fdb1e96e2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per = frac_FG_CO2_map_0.frac_FG_CO2_cumul.max().values.item()\n",
    "ranges = np.arange(0, 2, 0.01)\n",
    "#ranges = np.array([1e-3, 1e-2, 1e-1, 0.5, 1e0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6150cf-5d8c-455e-92c5-6ce47f1ee181",
   "metadata": {},
   "source": [
    "Given ranges, calculate sums inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b509-845c-4f1f-adc4-3ae914ac6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_inside(frac_FG_CO2_map_63, ranges):\n",
    "    '''\n",
    "    Given a list of max_values, return the sums inside\n",
    "    '''\n",
    "    data_array = frac_FG_CO2_map_63.frac_FG_CO2_cumul\n",
    "\n",
    "    l_sum_inside = []\n",
    "    for max_value in ranges:\n",
    "        selected_data = data_array.where( (data_array >= max_value), drop=True)\n",
    "        sum_inside = selected_data.sum(dim=['nlat', 'nlon']).values.item()\n",
    "        l_sum_inside.append(sum_inside)\n",
    "    return np.array(l_sum_inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2c801-84ad-4c03-bf2c-e1da588ab4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [1e-3, 0.008, 0.02, 0.1]\n",
    "print(get_sum_inside(frac_FG_CO2_map_0, ranges))\n",
    "print(get_sum_inside(frac_FG_CO2_map_63, ranges))\n",
    "print(get_sum_inside(frac_FG_CO2_map_142, ranges))\n",
    "print(get_sum_inside(frac_FG_CO2_map_129, ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951616d8-bf98-4e77-b78a-b2198ad3be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = get_sum_inside(frac_FG_CO2_map_0, ranges)\n",
    "int_array = list(map(int, arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e83c21-0222-4da9-b825-84c1ba3a0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9cfca-80a9-4b58-a66d-2e209e49997e",
   "metadata": {},
   "source": [
    "Given the sums, what should be the ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b53f0-c15a-479d-b885-a579cbc963a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_values(frac_FG_CO2_map_63, target_sums, thresholds):\n",
    "    '''\n",
    "    Given an array of target_sum, return the max values that will sum to the targets.\n",
    "    '''\n",
    "\n",
    "    l_max_value = []\n",
    "\n",
    "    data_array = frac_FG_CO2_map_63.frac_FG_CO2_cumul\n",
    "    \n",
    "    for target, thres in zip(target_sums, thresholds):\n",
    "\n",
    "        max_value = np.round(data_array.max().item(), 2) # start value\n",
    "        \n",
    "        #print(\"Target:\", target)\n",
    "        \n",
    "        # Iterate to find the largest max_value that makes the sum equal to the target value\n",
    "        while True:\n",
    "            \n",
    "            selected_data = data_array.where(data_array >= max_value, drop=True)\n",
    "            sum_inside = selected_data.sum(dim=['nlat', 'nlon']).values\n",
    "            \n",
    "            #print(max_value, sum_inside,target, np.abs(sum_inside - target))\n",
    "\n",
    "            if sum_inside - target >= thres:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Decrease max_value for the next iteration\n",
    "                max_value -= 0.005\n",
    "        \n",
    "        l_max_value.append(max_value)\n",
    "        #print(\"Largest max_values:\", l_max_value)\n",
    "    #l_max_value.append(0.001)\n",
    "        \n",
    "    return l_max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d98e5-cc7c-4da3-aa11-35267ef1e110",
   "metadata": {},
   "source": [
    "Figure out the labels and ranges one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd244df8-eb20-4e29-baff-a44c0e678808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(frac_FG_CO2_map_0, the_num):\n",
    "    \n",
    "    data_array = frac_FG_CO2_map_0.frac_FG_CO2_cumul\n",
    "    selected_data = data_array.where( (data_array >= the_num), drop=True)\n",
    "    sum_inside = selected_data.sum(dim=['nlat', 'nlon']).values.item()\n",
    "    return sum_inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fb4f8-a0ee-4060-9ef5-270b8a2a0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max_value = []\n",
    "L_target_sums = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0874b-a982-4286-bebf-522cdd211c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sums = [30, 50, 70, ]\n",
    "thresholds = [0.3, 0.3, 0.3, ]\n",
    "l_max_value = get_max_values(frac_FG_CO2_map_0, target_sums, thresholds)\n",
    "\n",
    "the_num = 0.0135\n",
    "sum_inside = get_sum(frac_FG_CO2_map_0, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "the_num = 0.0065\n",
    "sum_inside = get_sum(frac_FG_CO2_map_0, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "l_max_value.reverse()\n",
    "target_sums.reverse()\n",
    "\n",
    "l_max_value, target_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024dc51d-21a7-4cd1-9fa7-5a61a72703d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max_value.append(l_max_value)\n",
    "L_target_sums.append(target_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57e462-e9de-411f-b221-e1a3aaf471a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sums = [30, 50, 70, ]\n",
    "thresholds = [0.3, 0.3, 1, ]\n",
    "l_max_value = get_max_values(frac_FG_CO2_map_63, target_sums, thresholds)\n",
    "\n",
    "the_num = 0.045\n",
    "sum_inside = get_sum(frac_FG_CO2_map_63, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "the_num = 0.0013\n",
    "sum_inside = get_sum(frac_FG_CO2_map_63, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "l_max_value.reverse()\n",
    "target_sums.reverse()\n",
    "\n",
    "l_max_value, target_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a606e4c-6841-4cd3-8cde-0f4d946ba859",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max_value.append(l_max_value)\n",
    "L_target_sums.append(target_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264cb97-157c-4f79-a38b-523d41f628c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sums = [30, 50, 70]\n",
    "thresholds = [0.3, 0.3, 0.3]\n",
    "l_max_value = get_max_values(frac_FG_CO2_map_142, target_sums, thresholds)\n",
    "\n",
    "the_num = 0.0041\n",
    "sum_inside = get_sum(frac_FG_CO2_map_142, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "the_num = 0.0022\n",
    "sum_inside = get_sum(frac_FG_CO2_map_142, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "l_max_value.reverse()\n",
    "target_sums.reverse()\n",
    "\n",
    "l_max_value, target_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb4b09-b71d-4bc8-9fb6-740460890fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max_value.append(l_max_value)\n",
    "L_target_sums.append(target_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856ea34-6b2a-42ee-b9d4-f131ad1bf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sums = [30, 50, 70]\n",
    "thresholds = [0.3, 0.3, 0.3]\n",
    "l_max_value = get_max_values(frac_FG_CO2_map_129, target_sums, thresholds)\n",
    "\n",
    "the_num = 0.0027\n",
    "sum_inside = get_sum(frac_FG_CO2_map_129, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "the_num = 0.0015\n",
    "sum_inside = get_sum(frac_FG_CO2_map_129, the_num)\n",
    "target_sums.append(sum_inside)\n",
    "l_max_value.append(the_num)\n",
    "\n",
    "l_max_value.reverse()\n",
    "target_sums.reverse()\n",
    "\n",
    "l_max_value, target_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfae9a-279c-414c-92eb-b8c76b927419",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max_value.append(l_max_value)\n",
    "L_target_sums.append(target_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff3294-fcb7-402d-b130-2c9153e311a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, ii):\n",
    "    # a list of colors\n",
    "    # colorsss = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # ind_color = np.arange(len(colorsss)) # 0- 9\n",
    "\n",
    "    vertices = np.array(final_polygon_vertices_atlantic[ii])\n",
    "    # plot convex hull\n",
    "    if len(vertices) >= 3:\n",
    "        hull = ConvexHull(vertices) # get the indices of the periphery of a patch, for plotting purpose\n",
    "        polygon = list(vertices[iii] for iii in hull.vertices.tolist() )\n",
    "        polygon = np.array(polygon)\n",
    "\n",
    "        ax.plot(np.append(polygon[:,0], polygon[0,0]), np.append(polygon[:,1], polygon[0,1]), 'k-', linewidth=0.5, alpha=0.3, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # plot polygon masks\n",
    "    index = np.where(final_polygon_mask_atlantic[ii] == 1)\n",
    "    ax.scatter(tlong[index], tlat[index], c='gray', s=1, alpha=0.3, transform=ccrs.PlateCarree())\n",
    "    #ax.text(cluster_centers_atlantic[ii, 0]-2, cluster_centers_atlantic[ii, 1]-1,str(ii), fontsize=9, color='k', transform=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d1217-a515-4d91-bc88-f52d014f9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ds_4poly.isel(polygon=i).mean(dim='season').FG_CO2_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b0f0b-0215-45cf-9b3e-3c8813129f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_dist(FG_CO2_rings_per, long=False):\n",
    "\n",
    "    all_rs = [3,6,12,24,72,180]\n",
    "    r=0\n",
    "    stacks = []\n",
    "    labels = []\n",
    "\n",
    "    for rs in all_rs:\n",
    "        # sum individual histograms across the time axis into groups.\n",
    "        sum_over_time = np.sum(FG_CO2_rings_per[r:rs],axis=0)*100\n",
    "        cum_sum = np.cumsum(sum_over_time)\n",
    "        stacks.append(cum_sum)\n",
    "        percentage = np.sum(cum_sum[-1])\n",
    "\n",
    "        if long==True:\n",
    "            labels.append(\"%.1f %% in months %d-%d\"%(percentage,r,rs))\n",
    "        else:\n",
    "            labels.append(\"%.1f %%\"%(percentage))\n",
    "        # move to the next block\n",
    "        #r=rs\n",
    "        \n",
    "    return stacks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edbd55-302d-40cc-9e54-f3b1f1616cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks, labels = get_cumulative_dist(temp, long=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1658c-4546-49dc-adbd-0ef8d34b19ff",
   "metadata": {},
   "source": [
    "# Fig. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8b90f-32c6-4b12-b0e6-695feb4207bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ticks = np.arange(0,5000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074df2b-2c59-4b9c-99d7-e1dd99a472a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss[0].pCO2_SURF_excess.max().values, dss[1].pCO2_SURF_excess.max().values, dss[2].pCO2_SURF_excess.max().values, dss[3].pCO2_SURF_excess.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff081a7-5dfe-4bfc-b19d-690a1f157fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, ii):\n",
    "    # a list of colors\n",
    "    # colorsss = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # ind_color = np.arange(len(colorsss)) # 0- 9\n",
    "\n",
    "    vertices = np.array(final_polygon_vertices_atlantic[ii])\n",
    "    # plot convex hull\n",
    "    if len(vertices) >= 3:\n",
    "        hull = ConvexHull(vertices) # get the indices of the periphery of a patch, for plotting purpose\n",
    "        polygon = list(vertices[iii] for iii in hull.vertices.tolist() )\n",
    "        polygon = np.array(polygon)\n",
    "\n",
    "        ax.plot(np.append(polygon[:,0], polygon[0,0]), np.append(polygon[:,1], polygon[0,1]), 'k-', linewidth=0.5, alpha=0.3, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # plot polygon masks\n",
    "    index = np.where(final_polygon_mask_atlantic[ii] == 1)\n",
    "    ax.scatter(tlong[index], tlat[index], c='gray', s=1, alpha=0.3, transform=ccrs.PlateCarree())\n",
    "    #ax.text(cluster_centers_atlantic[ii, 0]-2, cluster_centers_atlantic[ii, 1]-1,str(ii), fontsize=9, color='k', transform=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e279ed-cbde-4b79-ac9b-82b6fd3c5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ad2ee-beaa-4657-83e9-f14d50fbee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "gs = gridspec.GridSpec(4, 3, width_ratios=[1, 1, 2])\n",
    "\n",
    "colors = ['blue', 'cyan', 'red', 'orange']\n",
    "labels = ['Jan', 'Apr', 'Jul', 'Oct']\n",
    "lon_mins = [-80, -80, -100, -100]\n",
    "lon_maxs = [25, 25, 25, 25]\n",
    "lat_mins = [0, 0, 0, -45]\n",
    "lat_maxs = [80, 80, 80, 47]\n",
    "central_longitudes = [0, 0, 0, 0]\n",
    "region_names  = ['Labrador Sea','St. Lawrence Gulf','Subtrop. N. Atlantic','Equat. Atlantic']\n",
    "\n",
    "def modify_ax_alk(ax):\n",
    "    ax.set_ylim(5000, 1)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    custom_y_ticks = [10, 100, 500, 1000, 4000]\n",
    "    custom_y_labels  = [str(num) for num in custom_y_ticks]\n",
    "    ax.set_yticks(custom_y_ticks)\n",
    "    ax.set_yticklabels(custom_y_labels);\n",
    "    \n",
    "    ax.set_xlim(-0.01, 0.6)\n",
    "    custom_x_ticks = np.arange(0, 0.7, 0.1)\n",
    "    custom_x_labels  = [0, 0.1, 0, 0.1, 0, 0.1, 0.2]\n",
    "    custom_x_labels = [str(num) for num in custom_x_labels]\n",
    "    ax.set_xticks(custom_x_ticks)\n",
    "    ax.set_xticklabels(custom_x_labels);\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    oae_ = oae_effs[i] # OAE_result for a region\n",
    "    frac_alk_ = fra_alks[i] # frac_alk for a region\n",
    "    surf_dil = all_curves_global.sel(region=reg, polygon=polys[i])  # surface dilution curves for a polygon, with 4 seasons\n",
    "    tau_comp = all_curves_global_tau_FG_CO2.sel(region=reg, polygon=polys[i]) # tau gax componets for a region\n",
    "\n",
    "    \n",
    "    for j in range(5):\n",
    "        if j == 0:\n",
    "            #ax = fig.add_subplot(4, 3, i*3+j+1, projection=ccrs.PlateCarree(central_longitude=central_longitudes[i]))\n",
    "            ax = plt.subplot(gs[i, j], projection=ccrs.PlateCarree(central_longitude=central_longitudes[i]))\n",
    "            ax.text(-0.4, 0.5, f'{region_names[i]}', ha='center', va='center', rotation=90, transform=ax.transAxes, fontsize=17)  \n",
    "\n",
    "            ### add map\n",
    "            ds_ = util.pop_add_cyclic(dss[i])\n",
    "\n",
    "            lon_min = lon_mins[i]\n",
    "            lon_max = lon_maxs[i]\n",
    "            lat_min = lat_mins[i]\n",
    "            lat_max = lat_maxs[i]\n",
    "\n",
    "            custom_colorbar_ticks = [1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "            negated_numbers = [-num for num in custom_colorbar_ticks]\n",
    "            custom_colorbar_labels  = [str(num) for num in negated_numbers]\n",
    "\n",
    "            sca = ax.contourf(ds_.TLONG, ds_.TLAT, -ds_.pCO2_SURF_excess,\n",
    "                              transform=ccrs.PlateCarree(),\n",
    "                              cmap=plt.cm.cool,\n",
    "\n",
    "                              levels = custom_colorbar_ticks,\n",
    "                              # #levels=[-1e1, -1e0, -1e-1, -1e-2, -1e-3, -1e-4],\n",
    "                              extend='max',\n",
    "                              norm=LogNorm(),              \n",
    "                             );\n",
    "            def modify(ax):\n",
    "                ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "                ax.set_xticks(np.arange(lon_min+20, lon_max, 60), crs=ccrs.PlateCarree())\n",
    "                ax.set_yticks(np.arange( lat_min+30, lat_max, 30), crs=ccrs.PlateCarree())\n",
    "                lon_formatter = LongitudeFormatter(zero_direction_label=False)\n",
    "                lat_formatter = LatitudeFormatter()\n",
    "                ax.xaxis.set_major_formatter(lon_formatter)\n",
    "                ax.yaxis.set_major_formatter(lat_formatter) \n",
    "\n",
    "                ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "                ###### colorbar\n",
    "                if i == 0:\n",
    "                    cax = fig.add_axes([-0.27, 0.085, 1, 0.05])  # left, bottom, width, height\n",
    "                    cb = fig.colorbar(sca, ax=cax, shrink=1, orientation='horizontal')\n",
    "                    cb.ax.set_title('Surface $pCO_2$ deficit (µatm)', y=-6, fontsize=13)\n",
    "                    cb.set_ticks(custom_colorbar_ticks)\n",
    "                    cb.set_ticklabels(custom_colorbar_labels)\n",
    "                    cb.ax.tick_params(axis='x', labelsize=11)\n",
    "                \n",
    "            modify(ax)\n",
    "            \n",
    "            ## add polygons\n",
    "            if i == 0:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 0)\n",
    "                ax.text(-108, 82, 'a, i', fontsize=16, fontweight='bold')\n",
    "            elif i == 1:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 63)\n",
    "                ax.text(-108, 82, 'b, i', fontsize=16, fontweight='bold')\n",
    "            elif i == 2:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 142)\n",
    "                ax.text(-128, 84, 'c, i', fontsize=16, fontweight='bold')\n",
    "            elif i == 3:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 129)\n",
    "                ax.text(-128, 52, 'd, i', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        ################ oae eff\n",
    "        elif j == 1:\n",
    "            #ax = fig.add_subplot(4, 3, i*3+j+1)\n",
    "            ax = plt.subplot(gs[i, j])\n",
    "            ax.text(-4, 0.9, 'ii', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            tarray = np.arange(0, 180, 1)/12\n",
    "            \n",
    "            for k in range(4):\n",
    "                ax.plot(tarray, no_nans(oae_[k].OAE_efficiency.values), linewidth=2, color=colors[k], label=labels[k])\n",
    "                ax.set_ylim(0, 0.9)\n",
    "                ax.set_ylabel('OAE efficiency')\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Time since release (years)')\n",
    "                custom_x_ticks = [0, 1, 2, 4, 8, 15]\n",
    "                custom_x_labels  = [str(num) for num in custom_x_ticks]\n",
    "                ax.set_xticks(custom_x_ticks)\n",
    "                ax.set_xticklabels(custom_x_labels);\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "\n",
    "            # add vertical lines\n",
    "            years_highlight = [1, 2, 8]\n",
    "            for p in range(len(years_highlight)):\n",
    "                ax.axvline(years_highlight[p], linestyle='--', color='k',linewidth=1, )\n",
    "                \n",
    "            if i == 1:\n",
    "                ax.legend(loc='lower right')\n",
    "\n",
    "        ################ vertical profiles of excess alk\n",
    "        elif j == 2:\n",
    "            #ax = fig.add_subplot(4, 3, i*3+j+1)\n",
    "            ax = plt.subplot(gs[i, j])\n",
    "            ax.text(-0.05, 1.2, 'iii', fontsize=16, fontweight='bold')\n",
    "            depth_array = frac_alk_invertical.z_t.values/100  # m\n",
    "            \n",
    "            mld_ = eff_mld[i] # mld for this polygon\n",
    "            for k in range(4): # for a season\n",
    "                ax.plot(frac_alk_[k].frac_alk.isel(time=12*1), depth_array, linewidth=2, color=colors[k], label=labels[k], linestyle='-')\n",
    "                ax.plot(frac_alk_[k].frac_alk.isel(time=12*2) + 0.2, depth_array, linewidth=2, color=colors[k], linestyle='-')\n",
    "                ax.plot(frac_alk_[k].frac_alk.isel(time=12*8) + 0.4, depth_array, linewidth=2, color=colors[k], linestyle='-')\n",
    "                \n",
    "                # add mld for year 1, 2, 8\n",
    "                ax.plot([0, 0.2-0.04], [mld_[k,0], mld_[k,0]], color=colors[k], linestyle=':', linewidth=2)\n",
    "                ax.plot([0.2, 0.4-0.04], [mld_[k,1], mld_[k,1]], color=colors[k], linestyle=':', linewidth=2)\n",
    "                ax.plot([0.4, 0.6-0.04], [mld_[k,2], mld_[k,2]], color=colors[k], linestyle=':', linewidth=2)\n",
    "                \n",
    "                \n",
    "            modify_ax_alk(ax)\n",
    "            ax.set_ylabel('Depth (m)')\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Fraction of total excess Alk')\n",
    "            else:\n",
    "                ax.set_xticklabels([]);\n",
    "            if i == 0:\n",
    "                #ax.legend(loc='upper right')\n",
    "                ax.text(0.02, 4, f'After 1 year')\n",
    "                ax.text(0.22, 4, f'2 years')\n",
    "                ax.text(0.42, 4, f'8 years')\n",
    "                \n",
    "                \n",
    "                \n",
    "plt.subplots_adjust(wspace=0.35, hspace=0.2)\n",
    "\n",
    "#plt.savefig('./Figure4_frac_alk_new_weightedMLD_Feb15.png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd59b6b-33c0-4acb-8238-c9d44b9add62",
   "metadata": {},
   "source": [
    "# Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e6b5d-8e47-46ad-952f-6054c7f6ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b801d00-55e9-4c69-9316-b6fe29a89396",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_ds = xr.open_dataset('./data/Uptake_percent_maps_all.nc')\n",
    "\n",
    "mean_over_season = whole_ds.mean(dim='season')\n",
    "std_over_season = whole_ds.std(dim='season')\n",
    "\n",
    "mean_over_season = util.pop_add_cyclic(mean_over_season)\n",
    "std_over_season = util.pop_add_cyclic(std_over_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590ec1f-52dd-4fbe-9850-412b92d44d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig = plt.figure(figsize=(15,11))\n",
    "gs = gridspec.GridSpec(4, 3, width_ratios=[2, 1, 1])\n",
    "\n",
    "# Separate the first column into 3 rows\n",
    "gs_sub1 = gs[:, 0].subgridspec(3, 1, height_ratios=[1, 1, 1], hspace=0.1)\n",
    "\n",
    "# Now you can access the subgridspec for the first column like this:\n",
    "ax1 = plt.subplot(gs_sub1[0, 0])\n",
    "ax2 = plt.subplot(gs_sub1[1, 0])\n",
    "ax3 = plt.subplot(gs_sub1[2, 0])\n",
    "\n",
    "\n",
    "################## global maps\n",
    "time_window = 180\n",
    "dist2center = 1000\n",
    "\n",
    "FONTSIZE = 14\n",
    "def modify(ax):\n",
    "    ax.set_extent([0, 360, -85, 80], crs=ccrs.PlateCarree())\n",
    "    #ax.stock_img()\n",
    "    ax.imshow(imread('./lightearth.jpg'),origin='upper', transform=ccrs.PlateCarree(), extent=[-180, 180, -90, 90])\n",
    "    lon_formatter = LongitudeFormatter(zero_direction_label=False)\n",
    "    lat_formatter = LatitudeFormatter()\n",
    "    ax.xaxis.set_major_formatter(lon_formatter)\n",
    "    ax.yaxis.set_major_formatter(lat_formatter) \n",
    "\n",
    "central_longitude=208\n",
    "\n",
    "dist2center = [500, 1000, 2000]\n",
    "labels = ['a  500 km', 'b  1000 km', 'c  2000 km']\n",
    "\n",
    "for i in range(3):\n",
    "    ax = plt.subplot(gs_sub1[i, 0],  projection=ccrs.PlateCarree(central_longitude=central_longitude))\n",
    "    #ax = fig.add_subplot(2, 2, i+1, projection=ccrs.PlateCarree(central_longitude=central_longitude))\n",
    "    ax.pcolormesh(mean_over_season.TLONG, mean_over_season.TLAT, mean_over_season.sel(time_window=time_window, dist2center=dist2center[i]).uptake_percent, transform=ccrs.PlateCarree(), cmap='rainbow', vmin=0, vmax=100)\n",
    "    \n",
    "    ax.text(40, 60, labels[i][3:], fontsize=FONTSIZE, transform=ccrs.PlateCarree(), fontweight='bold')\n",
    "    ax.text(30, 85, labels[i][:1], fontsize=FONTSIZE, transform=ccrs.PlateCarree(), fontweight='bold')\n",
    "    ax.set_yticks([-60,-30,0,30,60], crs=ccrs.PlateCarree())\n",
    "    ax.set_yticklabels(ax.get_yticks(), fontsize=FONTSIZE)\n",
    "    ax.set_xticks(np.arange(0, 360, 60), crs=ccrs.PlateCarree())\n",
    "    ax.set_xticklabels(ax.get_xticks(), fontsize=FONTSIZE)\n",
    "        \n",
    "    modify(ax)\n",
    "    \n",
    "def add_colorbar(x0, y0, vmin, vmax, label, num_levels_ticks, cmap_label='rainbow'):\n",
    "    '''\n",
    "    x0, y0: start location for the colorbar\n",
    "    vmin, vmax: range of the colorbar\n",
    "    label: label of the colorbar'\n",
    "    '''\n",
    "    cax = fig.add_axes([x0, y0, 0.2, 0.02])  # [x0, y0, width, height]\n",
    "    cmap = plt.colormaps[cmap_label]\n",
    "    normalize = plt.Normalize(vmin=vmin, vmax=vmax)  # Normalize the color values\n",
    "    sm = cm.ScalarMappable(cmap=cmap, norm=normalize)\n",
    "    cbar = fig.colorbar(sm, cax=cax, shrink=0.5, label=label, orientation='horizontal', ticks=np.linspace(vmin, vmax, num_levels_ticks))\n",
    "    cbar.ax.tick_params(labelsize=FONTSIZE)\n",
    "    cbar.ax.xaxis.label.set_size(FONTSIZE)\n",
    "\n",
    "add_colorbar(0.18, 0.07, 0, 100, '% of $CO_2$ uptake', 6)\n",
    "\n",
    "\n",
    "##################################### last 2 columns\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "labels = ['Jan', 'Apr', 'Jul', 'Oct']\n",
    "lon_mins = [-80, -80, -100, -100]\n",
    "lon_maxs = [25, 25, 25, 25]\n",
    "lat_mins = [0, 0, 0, -45]\n",
    "lat_maxs = [80, 80, 80, 47]\n",
    "central_longitudes = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "def modify_ax_alk(ax):\n",
    "    ax.set_ylim(5000, 1)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    custom_y_ticks = [10, 100, 500, 1000, 4000]\n",
    "    custom_y_labels  = [str(num) for num in custom_y_ticks]\n",
    "    ax.set_yticks(custom_y_ticks)\n",
    "    ax.set_yticklabels(custom_y_labels);\n",
    "    \n",
    "    ax.set_xlim(-0.01, 0.6)\n",
    "    custom_x_ticks = np.arange(0, 0.7, 0.1)\n",
    "    custom_x_labels  = [0, 0.1, 0, 0.1, 0, 0.1, 0.2]\n",
    "    custom_x_labels = [str(num) for num in custom_x_labels]\n",
    "    ax.set_xticks(custom_x_ticks)\n",
    "    ax.set_xticklabels(custom_x_labels);\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    oae_ = oae_effs[i] # OAE_result for a region\n",
    "    frac_alk_ = fra_alks[i] # frac_alk for a region\n",
    "    surf_dil = all_curves_global.sel(region=reg, polygon=polys[i])  # surface dilution curves for a polygon, with 4 seasons\n",
    "    tau_comp = all_curves_global_tau_FG_CO2.sel(region=reg, polygon=polys[i]) # tau gax componets for a region\n",
    "\n",
    "    \n",
    "    for j in range(3):\n",
    "       \n",
    "        if j == 2:\n",
    "            \n",
    "            ax = plt.subplot(gs[i, j], projection=ccrs.PlateCarree(central_longitude=central_longitudes[i]))\n",
    "            \n",
    "            ### add map\n",
    "            ds_ = util.pop_add_cyclic(ds_frac_FG_CO2_map[i])\n",
    "\n",
    "            lon_min = lon_mins[i]\n",
    "            lon_max = lon_maxs[i]\n",
    "            lat_min = lat_mins[i]\n",
    "            lat_max = lat_maxs[i]\n",
    "            \n",
    "            custom_colorbar_ticks = L_max_value[i]\n",
    "\n",
    "            sca = ax.contour(ds_.TLONG, ds_.TLAT, ds_.frac_FG_CO2_cumul,\n",
    "                              transform=ccrs.PlateCarree(),\n",
    "                              cmap=plt.cm.RdYlGn,\n",
    "                              levels = custom_colorbar_ticks,\n",
    "                              extend='both',\n",
    "                              #norm=LogNorm(),           \n",
    "                             );\n",
    "\n",
    "            # Specify different colors for each level\n",
    "            contour_colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "            #contour_colors.reverse()\n",
    "            for o, collection in enumerate(sca.collections):\n",
    "                collection.set_edgecolor(contour_colors[o])\n",
    "            \n",
    "            #### add numbers in contour lines\n",
    "            # arr = get_sum_inside(ds_, ranges)\n",
    "            # int_array = list(map(int, arr))\n",
    "\n",
    "            \n",
    "            fmt = {}\n",
    "            strs = [str(ele)+'%' for ele in L_target_sums[i]]\n",
    "            print(strs)\n",
    "            for l, s in zip(sca.levels, strs):\n",
    "                fmt[l] = s\n",
    "            \n",
    "            # Label every other level using strings\n",
    "            ax.clabel(sca, sca.levels, inline=True, fmt=fmt, fontsize=20, inline_spacing=3, colors='k')\n",
    "\n",
    "                    \n",
    "            def modify(ax):\n",
    "                ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "                ax.set_xticks(np.arange(lon_min+20, lon_max, 60), crs=ccrs.PlateCarree())\n",
    "                ax.set_yticks(np.arange( lat_min+30, lat_max, 30), crs=ccrs.PlateCarree())\n",
    "                lon_formatter = LongitudeFormatter(zero_direction_label=False)\n",
    "                lat_formatter = LatitudeFormatter()\n",
    "                ax.xaxis.set_major_formatter(lon_formatter)\n",
    "                ax.yaxis.set_major_formatter(lat_formatter) \n",
    "                #ax.imshow(imread('./lightearth.jpg'),origin='upper', transform=ccrs.PlateCarree(), extent=[-180, 180, -90, 90])\n",
    "\n",
    "                ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "            modify(ax)\n",
    "            ## add polygons\n",
    "            if i == 0:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 0)\n",
    "                ax.text(-100, 78, 'ii', fontsize=FONTSIZE, fontweight='bold')\n",
    "            elif i == 1:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 63)\n",
    "                ax.text(-100, 78, 'ii', fontsize=FONTSIZE, fontweight='bold')\n",
    "            elif i == 2:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 142)\n",
    "                ax.text(-120, 82, 'ii', fontsize=FONTSIZE, fontweight='bold')\n",
    "            elif i == 3:\n",
    "                plot_polygons(ax, final_polygon_mask_atlantic, final_polygon_vertices_atlantic, cluster_centers_atlantic, 129)\n",
    "                ax.text(-120, 52, 'ii', fontsize=FONTSIZE, fontweight='bold')\n",
    "                percents = [30, 50, 70, 90, 95]\n",
    "                for kk in range(5):\n",
    "                    ax.text(-120 + kk*35, -80, f'{percents[kk]}%', fontsize=16, fontweight='bold', color=contour_colors[4-kk])\n",
    "\n",
    "                \n",
    "        ################## seasonla mean, CO2 uptake histogram\n",
    "        elif j == 1:\n",
    "       \n",
    "            ax = plt.subplot(gs[i, j])\n",
    "            \n",
    "            temp = ds_4poly.isel(polygon=i).mean(dim='season').FG_CO2_percent.values\n",
    "            if i == 0:\n",
    "                stacks, _ = get_cumulative_dist(temp, long=True)\n",
    "            else:\n",
    "                stacks, _ = get_cumulative_dist(temp)\n",
    "\n",
    "            legends = ['3 months', '6 months', '1 year', '2 years','3 years', '15 years']\n",
    "            for jj in range(len(stacks)):\n",
    "                ax.plot(bin_edges_0/1000, stacks[jj], label=legends[jj])\n",
    "\n",
    "            ### add dashed line\n",
    "            ind_1000 = np.where(bin_edges_0/1000 == 1000)\n",
    "            ax.plot([1000, 1000], [0, stacks[-1][ind_1000]], 'k--')\n",
    "            ax.plot([0, 1000], [stacks[-1][ind_1000], stacks[-1][ind_1000]], 'k--')\n",
    "\n",
    "            ind_2000 = np.where(bin_edges_0/1000 == 2000)\n",
    "            ax.plot([2000, 2000], [0, stacks[-1][ind_2000]], 'k--')\n",
    "            ax.plot([0, 2000], [stacks[-1][ind_2000], stacks[-1][ind_2000]], 'k--')\n",
    "\n",
    "            print('Percentage in 1000, 2000 km', stacks[-1][ind_1000], stacks[-1][ind_2000])\n",
    "\n",
    "            if i == 1:\n",
    "                ax.legend(fontsize=11, loc='lower right')\n",
    "            \n",
    "            ax.set_xlim(-60,4000)\n",
    "            \n",
    "            ax.set_xticklabels('')\n",
    "            ax.set_ylim(-0.2, 100)\n",
    "            if i == 3:\n",
    "                ax.set_xlabel('Distance from addition center (km)')\n",
    "                ax.set_xticks(np.arange(0,5000,1000))\n",
    "                ax.set_xticklabels(np.arange(0,5000,1000))\n",
    "            \n",
    "            ax.set_yticks(np.arange(0,125,25))\n",
    "            ax.set_yticklabels(np.arange(0,125,25))\n",
    "        \n",
    "            if i == 0:\n",
    "                ax.text(-1500, 105, 'd, i', fontsize=FONTSIZE, fontweight='bold')\n",
    "            elif i == 1:\n",
    "                ax.text(-1500, 105, 'e, i', fontsize=FONTSIZE, fontweight='bold')\n",
    "            elif i == 2:\n",
    "                ax.text(-1500, 105, 'f, i', fontsize=FONTSIZE, fontweight='bold')\n",
    "            elif i == 3:\n",
    "                ax.text(-1500, 105, 'g, i', fontsize=FONTSIZE, fontweight='bold')\n",
    "            \n",
    "            ax.set_ylabel('% of $CO_2$ uptake')\n",
    "\n",
    "            \n",
    "                \n",
    "plt.subplots_adjust(wspace=0.35, hspace=0.2)\n",
    "\n",
    "#plt.savefig('./Figure4_percent_uptake_Feb15.png', dpi=400, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
